from pathlib import Path
import fitz
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

def preprocess_yujin(pdf_dir, persist_directory) :
    documents = load_yujin_pdf(pdf_dir)

    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500, 
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""] # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ìë¥´ë„ë¡ ìœ ë„
    )

    split_docs = text_splitter.split_documents(documents)
    print(f"\nâœ‚ï¸ ì´ {len(documents)}ê°œì˜ ì„¤êµë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬(ì¡°ê°)ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    if split_docs:
        print("ğŸ’¾ ChromaDBì— ì €ì¥ ì¤‘...")

        # ì„ë² ë”© ëª¨ë¸ ëª…ì‹œ (ê²€ìƒ‰ ë•Œì™€ ë™ì¼í•˜ê²Œ!)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name="yujin_works" # "pastor_sermons"
        )
        print("âœ… ì €ì¥ ì™„ë£Œ! DBê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")


def load_yujin_pdf(pdf_dir) : 
    print("ğŸ“‚ PDF ì²˜ë¦¬ ì‹œì‘...")
    documents = [] # LangChain Document ê°ì²´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
    
    if pdf_dir.exists():
        for pdf_path in pdf_dir.glob("*.pdf"):
            title, content = extract_core_sermon(pdf_path)

            if content:
                
                print(f" - [{title}] ë¡œë“œ ì™„ë£Œ ({len(content)}ì)")
                print(f" ğŸ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘: {pdf_path.name}")

                doc = Document(
                    page_content=content,
                    metadata={
                        "source": pdf_path.name, # íŒŒì¼ëª…
                        "title": title,          # ì„¤êµ ì œëª© (ë‹µë³€ ì¶œì²˜ í‘œê¸°ì— ì‚¬ìš©ë¨)
                        "author": "ê¹€ìœ ì§„ ëª©ì‚¬",  # í•„í„°ë§ìš©
                        "category": "sermon",
                    }
                )
                documents.append(doc)
            else:
                print(f" âš ï¸ ìŠ¤í‚µ: {pdf_path.name} (ì„œë¡ /ì¶•ë„ íŒ¨í„´ ë¯¸ë°œê²¬)")

    return documents

        
def extract_core_sermon(pdf_path):
    doc = fitz.open(pdf_path)
    
    # [ìµœì í™” 1] ì œëª© ì¶”ì¶œ: ì „ì²´ë¥¼ ì½ì§€ ì•Šê³  'ì²« í˜ì´ì§€'ë§Œ ì½ì–´ì„œ í•´ê²°
    title = ""
    try:
        if len(doc) > 0:
            first_page_text = doc[0].get_text("text")
            # ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¤‘ ê³µë°±ì´ ì•„ë‹Œ ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ ê°„ì£¼
            for line in first_page_text.splitlines():
                if line.strip():
                    title = line.strip()
                    break
    except Exception as e:
        print(f"ì œëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        title = "ì œëª© ì—†ìŒ"

    # 1. ì „ì²´ í…ìŠ¤íŠ¸ ë³‘í•© (ë³¸ë¬¸ ì¶”ì¶œìš©)
    full_text = ""
    for page in doc:
        full_text += page.get_text("text") + "\n"
    
    doc.close() # ë‹¤ ì¼ìœ¼ë‹ˆ ë‹«ê¸°

    if not full_text : 
        return None, None 

    # 2. ìœ„ì¹˜ ì°¾ê¸°
    start_keyword = "ì„œë¡ "
    end_keywords = ["ì¶•ë„", "ê¸°ë„"]
    
    start_index = full_text.find(start_keyword)

    # ì„œë¡ ì´ ì—†ìœ¼ë©´ ì‹¤íŒ¨
    if start_index == -1:
        return None, None

    # [ìµœì í™” 2] ë ìœ„ì¹˜ ì°¾ê¸°
    # ì „ì²´ì—ì„œ ì°¾ëŠ” ê²Œ ì•„ë‹ˆë¼, 'ì„œë¡ 'ì´ ë‚˜ì˜¨ ìœ„ì¹˜(start_index) 'ì´í›„'ë¶€í„° ì°¾ìŠµë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•´ì•¼ ì„œë¡ ë³´ë‹¤ ì•ì— ìˆëŠ”(ì˜ˆ: ì˜ˆë°°ìˆœì„œì§€ì˜ 'ê¸°ë„') ë‹¨ì–´ì— ë‚šì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
    found_end_indices = []
    for k in end_keywords:
        idx = full_text.find(k, start_index) # start_index ì´í›„ë¶€í„° ê²€ìƒ‰
        if idx != -1:
            found_end_indices.append(idx)

    # ë í›„ë³´ë“¤ ì¤‘ ê°€ì¥ ë¨¼ì € ë‚˜ì˜¤ëŠ” ê²ƒ ì„ íƒ (min)
    if found_end_indices:
        end_index = min(found_end_indices)
        
        # 3. ìŠ¬ë¼ì´ì‹± (ì œëª©ê³¼ ë³¸ë¬¸ ë°˜í™˜)
        # start_indexë¶€í„° end_indexê¹Œì§€
        core_content = full_text[start_index:end_index]
        return title, core_content.strip()
    
    else:
        # ì„œë¡ ì€ ì°¾ì•˜ëŠ”ë° ëë‚˜ëŠ” ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° (ë¬¸ì„œ ëê¹Œì§€ ê°€ì ¸ì˜´)
        # return title, full_text[start_index:].strip()
        return None, None