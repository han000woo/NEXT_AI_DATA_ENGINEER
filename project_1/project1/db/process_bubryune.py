from pathlib import Path
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain_core.documents import Document
from openai import OpenAI
from youtube_transcript_api import YouTubeTranscriptApi
import yt_dlp
import time
import random


def polite_sleep(min_sec=2.0, max_sec=4.0):
    delay = random.uniform(min_sec, max_sec)
    print(f"ğŸ˜´ {delay:.2f}ì´ˆ ëŒ€ê¸°")
    time.sleep(delay)


load_dotenv()

client = OpenAI()
# ì¦‰ë¬¸ ì¦‰ì„¤ ë ˆì „ë“œ 7í¸
# playlist_url = "https://www.youtube.com/playlist?list=PLeyE__d-DACMeLChk7JatWNZmMwv3M5f4"
# ì¦‰ë¬¸ ì¦‰ì„¤ 182í¸
playlist_url = (
    "https://www.youtube.com/playlist?list=PLeyE__d-DACM8w9c6ZAUiBkOj-B4UE8CB"
)

jukmun_dir = Path("jeukmun_transcripts")


def get_video_ids():
    ydl_opts = {
        "quiet": True,
        "extract_flat": True,  # â— ì¤‘ìš” (ë©”íƒ€ë°ì´í„°ë§Œ)
        "skip_download": True,
    }

    video_ids = []

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(playlist_url, download=False)

        for entry in info["entries"]:
            if entry and "id" in entry:
                video_ids.append(entry["id"])

    print(f"ì´ ì˜ìƒ ìˆ˜: {len(video_ids)}")
    return video_ids


def saving_txts(video_ids):
    jukmun_dir.mkdir(exist_ok=True)

    for idx, video_id in enumerate(video_ids, 1):
        print(f"\nâ–¶ [{idx}/{len(video_ids)}] {video_id}")
        ytt_api = YouTubeTranscriptApi()

        try:
            transcript = ytt_api.fetch(video_id, languages=["ko"])

            text = "\n".join(snippet.text for snippet in transcript)

            file_path = jukmun_dir / f"{idx:03d}_{video_id}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(text)

            print(f"âœ… ì €ì¥ ì™„ë£Œ: {file_path}")

        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {video_id} / {e}")

            # â— ì‹¤íŒ¨ ì‹œ ë” ê¸´ ëŒ€ê¸°
            polite_sleep(5, 8)
            continue

        # âœ… ì •ìƒ ì²˜ë¦¬ í›„ ê¸°ë³¸ ëŒ€ê¸°
        polite_sleep(2, 4)

        # âœ… 20ê°œë§ˆë‹¤ ì¿¨ë‹¤ìš´
        if idx % 20 == 0:
            print("ğŸ§Š ì¿¨ë‹¤ìš´ íƒ€ì„ (15ì´ˆ)")
            time.sleep(15)


# ë²•ë¥œìŠ¤ë‹˜ ìœ íŠœë¸Œ ë°ì´í„° í¬ë¡¤ë§
# video_ids = get_video_ids()
# saving_txts(video_ids)

SYSTEM_PROMPT = """
ë„ˆëŠ” ë²•ë¥œìŠ¤ë‹˜ì˜ ì¦‰ë¬¸ì¦‰ì„¤ ëŒ€í™”ë¥¼ ì •ë¦¬í•˜ëŠ” ì¡°ë ¥ìì´ë‹¤.

ê·œì¹™:
1. ì›ë¬¸ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ì•ŠëŠ”ë‹¤
2. ë²•ë¥œìŠ¤ë‹˜ ê°™ì€ ì–´ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤
"""


def make_user_prompt(transcript: str) -> str:
    return f"""
ì•„ë˜ëŠ” ë²•ë¥œìŠ¤ë‹˜ì˜ ì¦‰ë¬¸ì¦‰ì„¤ ê°•ì—° ìë§‰ì´ë‹¤.

ì‘ì—… ì§€ì‹œ:
- ì§ˆë¬¸ìì˜ í•µì‹¬ ê³ ë¯¼ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ë¼
- ë²•ë¥œìŠ¤ë‹˜ì˜ ë‹µë³€ì„ 'ê¹¨ë‹¬ìŒ ì¤‘ì‹¬ ìš”ì•½'ìœ¼ë¡œ 3~5ë¬¸ì¥ ì •ë¦¬í•˜ë¼
- ì›ë¬¸ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ê³  ì˜ë¯¸ë§Œ ì¬êµ¬ì„±í•˜ë¼

ì¶œë ¥ í˜•ì‹:
Q: (ì§ˆë¬¸ì˜ í•µì‹¬)
A: (ìš”ì•½ëœ ë‹µë³€)

ìë§‰:
{transcript}
"""


def summarize_jeukmun(transcript: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": make_user_prompt(transcript)},
        ],
        temperature=0.3,  # ì°½ì‘ ìµœì†Œí™”, ìš”ì•½ ì•ˆì •ì„± â†‘
        max_tokens=500,
    )

    return response.choices[0].message.content


def summarize_bubryune_data():
    input_dir = Path("jeukmun_transcripts")
    output_dir = Path("jeukmun_summaries")
    output_dir.mkdir(exist_ok=True)

    for txt_file in input_dir.glob("*.txt"):
        raw_text = txt_file.read_text(encoding="utf-8")

        summary = summarize_jeukmun(raw_text)
        time.sleep(10)

        output_file = output_dir / txt_file.name
        output_file.write_text(summary, encoding="utf-8")

        print(f"âœ… ìš”ì•½ ì™„ë£Œ: {output_file.name}")


def preprocess_bubryune(bub_dir, persist_directory):
    print(f"ğŸ“‚ '{bub_dir}' í´ë”ì—ì„œ ë²•ë¥œìŠ¤ë‹˜ ì„¤êµ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    # ë¬¸ì„œ ë¡œë“œ
    # documents = DirectoryLoader(
    #     bub_dir,
    #     glob="*.txt",
    #     loader_cls=TextLoader,
    #     loader_kwargs={"encoding": "utf-8"}, 
    # ).load()
    # print(documents)
    documents = [] 
    files = list(Path(bub_dir).glob("*.txt"))

    # if not documents:
    if not files :
        print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    for file_path in files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            # íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            title = file_path.stem 
            print("ì¦‰ë¬¸ì¦‰ì„¤ "+file_path.name[:3]+"ê°•")
            doc = Document(
                page_content=content,
                metadata={
                    "source": "ì¦‰ë¬¸ì¦‰ì„¤"+file_path.name[:3]+"ê°•",      # íŒŒì¼ëª… (ì˜ˆ: sermon_01.txt)
                    "title": title,                # ì œëª© (ì˜ˆ: sermon_01)
                    "author": "ë²•ë¥œìŠ¤ë‹˜",           # ì‘ì„±ì ê³ ì •
                    "category": "sermon"  # ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ìš©
                }
            )
            documents.append(doc)
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path.name}): {e}")
    
    print(f"âœ… ì´ {len(documents)}ê°œì˜ ì„¤êµ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]
    )
    split_docs = text_splitter.split_documents(documents)
    print(f"\nâœ‚ï¸ ì´ {len(documents)}ê°œì˜ ì„¤êµë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # ë²¡í„° DB ì €ì¥
    if split_docs:
        print("ğŸ’¾ ChromaDBì— ì €ì¥ ì¤‘...")
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name="bubryune_works"
        )
        print("âœ… ì €ì¥ ì™„ë£Œ! DBê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
