import os
from pathlib import Path
import openai
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from enums.target import TARGET_CONFIG, AnswerTarget, SermonState
from dotenv import load_dotenv

BASE_DIR = Path(__file__).resolve().parents[1]
CONFIG_PATH = BASE_DIR / "config" / ".env"

load_dotenv(CONFIG_PATH)

# --- 1. ì„ë² ë”© ì„¤ì • (DB ë§Œë“¤ ë•Œì™€ ë™ì¼í•œ ëª¨ë¸ í•„ìˆ˜!) ---
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# --- 2. DB ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ) ---
# ê²½ë¡œê°€ ì‹¤ì œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ì´ ìˆìœ¼ë©´ ë” ì•ˆì „í•©ë‹ˆë‹¤.
DB_PATH = "chroma_vector_db" # ë˜ëŠ” "final_db" ë“± ì‹¤ì œ í´ë”ëª…

woonsung_vectorstore = Chroma(
    persist_directory=DB_PATH,
    embedding_function=embeddings,
    collection_name='woonsung_works'
)

yujin_vectorstore = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings,
    collection_name='yujin_works' # ingestí•  ë•Œ ì“´ ì´ë¦„ í™•ì¸!
)

nietzsche_vectorstore = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings,
    collection_name='nietzsche_works' # ingestí•  ë•Œ ì“´ ì´ë¦„ í™•ì¸!
)

def get_response(user_input, chat_history, target: AnswerTarget):
    """
    return: (ë‹µë³€ í…ìŠ¤íŠ¸, (ìƒíƒœ, ì¶œì²˜_í…ìŠ¤íŠ¸))
    """
    
    # --- 3. íƒ€ê²Ÿë³„ DB ë° ì„¤ì • ë¶„ê¸° ---
    if target == AnswerTarget.PASTOR_A:
        vectorstore = yujin_vectorstore
        author_title = "ê¹€ìœ ì§„ ëª©ì‚¬"
        # ì„¤êµ DBì—ì„œ ì œëª©ì„ ì°¾ëŠ” í‚¤ (ingest.py í™•ì¸ í•„ìš”)
        meta_key = 'title' 

    elif target == AnswerTarget.PASTOR_B:
        vectorstore = woonsung_vectorstore
        author_title = "ì •ìš´ì„± ëª©ì‚¬"
        # ì„¤êµ DBì—ì„œ ì œëª©ì„ ì°¾ëŠ” í‚¤ (ingest.py í™•ì¸ í•„ìš”)
        meta_key = 'title' 
    
    elif target == AnswerTarget.NIETZSCHE: # PHILOSOPHER_A ëŒ€ì‹  êµ¬ì²´ì  ëª…ì¹­ ê¶Œì¥
        vectorstore = nietzsche_vectorstore
        author_title = "ì² í•™ì"
        # ë‹ˆì²´ DBì—ì„œ ì œëª©ì„ ì°¾ëŠ” í‚¤ (ì•„ê¹Œ full_refë¡œ ì €ì¥í•¨)
        meta_key = 'full_ref' 

    else:
        # ì˜ˆì™¸ ì²˜ë¦¬: ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ê²Ÿì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        vectorstore = yujin_vectorstore
        author_title = "AI"
        meta_key = 'source'

    config = TARGET_CONFIG[target]
    author_name = config.get("name", "ìƒë‹´ê°€")

    # --- 4. ê²€ìƒ‰ (Filter ì œê±° ë° kê°’ ì¡°ì •) ---
    # ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ filter ë¶ˆí•„ìš”
    retriever = vectorstore.as_retriever(
        search_kwargs={
            "k": 3, # ë¬¸ë§¥ì„ ì¢€ ë” í’ë¶€í•˜ê²Œ í•˜ê¸° ìœ„í•´ 2 -> 3ìœ¼ë¡œ ì¦ê°€ ì¶”ì²œ
        }
    )
    
    docs = retriever.invoke(user_input)
    
    # --- 5. ê²€ìƒ‰ ê²°ê³¼ ë° ì¶œì²˜ ì •ë¦¬ ---
    if docs:
        context_text = "\n\n".join([doc.page_content for doc in docs])
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶œì²˜ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 'ì¶œì²˜ ë¯¸ìƒ')
        sources = [doc.metadata.get(meta_key, 'ì¶œì²˜ ë¯¸ìƒ') for doc in docs]
        unique_sources = list(set(sources)) # ì¤‘ë³µ ì œê±°
        
        # ì¶œì²˜ ë¬¸êµ¬ ìƒì„± (í˜ë¥´ì†Œë‚˜ì— ë§ê²Œ)
        if target == AnswerTarget.NIETZSCHE:
             source_str = f"ğŸ“œ ë‹ˆì²´ ì €ì„œ ì¸ìš©: {', '.join(unique_sources)}"
        else:
             source_str = f"ğŸ“– {author_name} {author_title} ì„¤êµ: {', '.join(unique_sources)}"

        source_info = (SermonState.FOUND, source_str)
        
        # ê²€ìƒ‰ëœ ë‚´ìš©ì´ ìˆì„ ë•Œ í”„ë¡¬í”„íŠ¸
        rag_prompt = (
            f"ë‹¤ìŒì€ ë‹¹ì‹ ì´ ì°¸ê³ í•´ì•¼ í•  ì§€ì‹ ë² ì´ìŠ¤(Context)ì…ë‹ˆë‹¤:\n"
            f"---------------------\n"
            f"{context_text}\n"
            f"---------------------\n"
            f"ìœ„ ì§€ì‹ ë² ì´ìŠ¤ì˜ ë‚´ìš©ê³¼ ë‹¹ì‹ ì˜ ì‚¬ìƒì„ ì—°ê²°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
            f"ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  ë‹¹ì‹ ì˜ ì² í•™ì  ê´€ì ì—ì„œ í•´ì„í•˜ì„¸ìš”."
        )

    else:
        context_text = ""
        source_info = (SermonState.NOT_FOUND, "") # -1 ëŒ€ì‹  ë¹ˆ ë¬¸ìì—´ ê¶Œì¥
        
        # ê²€ìƒ‰ëœ ë‚´ìš©ì´ ì—†ì„ ë•Œ í”„ë¡¬í”„íŠ¸
        rag_prompt = "ê´€ë ¨ëœ ë¬¸í—Œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ í‰ì†Œ ì‚¬ìƒê³¼ í†µì°°ë ¥ì— ì˜ì¡´í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."

    # --- 6. ìµœê·¼ ëŒ€í™” ì •ë¦¬ ---
    formatted_history = [
        {"role": msg["role"], "content": msg["content"]}
        for msg in chat_history[-6:] # í† í° ì ˆì•½ì„ ìœ„í•´ 10 -> 6 ì •ë„ë¡œ ì¡°ì ˆ (ì„ íƒì‚¬í•­)
    ]
    print(rag_prompt)

    # --- 7. ìµœì¢… ë©”ì‹œì§€ êµ¬ì„± ---
    system_message = {
        "role": "system",
        "content": f"{config['system_prompt']}\n\n[RAG ì§€ì¹¨]\n{rag_prompt}"
    }

    messages_to_send = [system_message] + formatted_history + [{"role": "user", "content": user_input}]

    # --- 8. API í˜¸ì¶œ ---
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages_to_send,
        temperature=0.7 
        # ë‹ˆì²´ì˜ ê²½ìš° ì°½ì˜ì„±ì„ ìœ„í•´ temperatureë¥¼ 0.8~0.9ë¡œ ë†’ì´ëŠ” ê²ƒë„ ë°©ë²•ì…ë‹ˆë‹¤.
    )

    return response.choices[0].message.content, source_info