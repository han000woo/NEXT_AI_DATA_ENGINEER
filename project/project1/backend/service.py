import os
from pathlib import Path
import openai
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from enums.target import TARGET_CONFIG, AnswerTarget, SermonState
from dotenv import load_dotenv

from utils.util import parse_list

BASE_DIR = Path(__file__).resolve().parents[1]
CONFIG_PATH = BASE_DIR / "config" / ".env"

load_dotenv(CONFIG_PATH)

# --- 1. ì„ë² ë”© ì„¤ì • (DB ë§Œë“¤ ë•Œì™€ ë™ì¼í•œ ëª¨ë¸ í•„ìˆ˜!) ---
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# --- 2. DB ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ) ---
DB_PATH = "chroma_vector_db" # ë˜ëŠ” "final_db" ë“± ì‹¤ì œ í´ë”ëª…

simple_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0) # ë¹„ìš© ì ˆê°ì„ ìœ„í•´ mini ëª¨ë¸ ê¶Œì¥

woonsung_vectorstore = Chroma(
    persist_directory=DB_PATH,
    embedding_function=embeddings,
    collection_name='woonsung_works'
)

yujin_vectorstore = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings,
    collection_name='yujin_works' # ingestí•  ë•Œ ì“´ ì´ë¦„ í™•ì¸!
)

nietzsche_vectorstore = Chroma(
    persist_directory=DB_PATH, 
    embedding_function=embeddings,
    collection_name='nietzsche_works' # ingestí•  ë•Œ ì“´ ì´ë¦„ í™•ì¸!
)

def get_response(user_input, chat_history, target: AnswerTarget):
    print("get_response")

    """
    return: (ë‹µë³€ í…ìŠ¤íŠ¸, (ìƒíƒœ, ì¶œì²˜_í…ìŠ¤íŠ¸))
    """
    
    # --- 3. íƒ€ê²Ÿë³„ DB ë° ì„¤ì • ë¶„ê¸° ---
    if target == AnswerTarget.PASTOR_A:
        vectorstore = yujin_vectorstore
        author_title = "ê¹€ìœ ì§„ ëª©ì‚¬"
        # ì„¤êµ DBì—ì„œ ì œëª©ì„ ì°¾ëŠ” í‚¤ (ingest.py í™•ì¸ í•„ìš”)
        meta_key = 'title' 

    elif target == AnswerTarget.PASTOR_B:
        vectorstore = woonsung_vectorstore
        author_title = "ì •ìš´ì„± ëª©ì‚¬"
        # ì„¤êµ DBì—ì„œ ì œëª©ì„ ì°¾ëŠ” í‚¤ (ingest.py í™•ì¸ í•„ìš”)
        meta_key = 'title' 
    
    elif target == AnswerTarget.NIETZSCHE: # PHILOSOPHER_A ëŒ€ì‹  êµ¬ì²´ì  ëª…ì¹­ ê¶Œì¥
        vectorstore = nietzsche_vectorstore
        author_title = "ì² í•™ì"
        # ë‹ˆì²´ DBì—ì„œ ì œëª©ì„ ì°¾ëŠ” í‚¤ (ì•„ê¹Œ full_refë¡œ ì €ì¥í•¨)
        meta_key = 'full_ref' 

    else:
        # ì˜ˆì™¸ ì²˜ë¦¬: ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ê²Ÿì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        vectorstore = yujin_vectorstore
        author_title = "AI"
        meta_key = 'source'

    config = TARGET_CONFIG[target]
    author_name = config.get("name", "ìƒë‹´ê°€")

    # --- 4. ê²€ìƒ‰ (Filter ì œê±° ë° kê°’ ì¡°ì •) ---

    bible_refs =  expect_query_bible_refs(user_input)
    docs_meta = vectorstore.similarity_search(
    user_input,
    k=3,
    filter={"bible_ref": {"$in": bible_refs}}
    )
    docs_all = vectorstore.similarity_search(
    user_input,
    k=3
    )
    
    docs = get_refined_docs(user_input, docs_meta, docs_all, simple_llm)
    
    # --- 5. ê²€ìƒ‰ ê²°ê³¼ ë° ì¶œì²˜ ì •ë¦¬ ---
    if docs:
        context_text = "\n\n".join([doc.page_content for doc in docs])
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶œì²˜ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 'ì¶œì²˜ ë¯¸ìƒ')
        sources = [doc.metadata.get(meta_key, 'ì¶œì²˜ ë¯¸ìƒ') for doc in docs]
        unique_sources = list(set(sources)) # ì¤‘ë³µ ì œê±°
        
        # ì¶œì²˜ ë¬¸êµ¬ ìƒì„± (í˜ë¥´ì†Œë‚˜ì— ë§ê²Œ)
        if target == AnswerTarget.NIETZSCHE:
             source_str = f"ğŸ“œ ë‹ˆì²´ ì €ì„œ ì¸ìš©: {', '.join(unique_sources)}"
        else:
             source_str = f"ğŸ“– {author_name} {author_title} ì„¤êµ: {', '.join(unique_sources)}"

        source_info = (SermonState.FOUND, source_str)
        
        # ê²€ìƒ‰ëœ ë‚´ìš©ì´ ìˆì„ ë•Œ í”„ë¡¬í”„íŠ¸
        rag_prompt = (
            f"ë‹¤ìŒì€ ë‹¹ì‹ ì´ ì°¸ê³ í•´ì•¼ í•  ì§€ì‹ ë² ì´ìŠ¤(Context)ì…ë‹ˆë‹¤:\n"
            f"---------------------\n"
            f"{context_text}\n"
            f"---------------------\n"
            f"ìœ„ ì§€ì‹ ë² ì´ìŠ¤ì˜ ë‚´ìš©ê³¼ ë‹¹ì‹ ì˜ ì‚¬ìƒì„ ì—°ê²°í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
            f"ì§€ì‹ ë² ì´ìŠ¤ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  ë‹¹ì‹ ì˜ ì² í•™ì  ê´€ì ì—ì„œ í•´ì„í•˜ì„¸ìš”."
        )

    else:
        context_text = ""
        source_info = (SermonState.NOT_FOUND, "") # -1 ëŒ€ì‹  ë¹ˆ ë¬¸ìì—´ ê¶Œì¥
        
        # ê²€ìƒ‰ëœ ë‚´ìš©ì´ ì—†ì„ ë•Œ í”„ë¡¬í”„íŠ¸
        rag_prompt = "ê´€ë ¨ëœ ë¬¸í—Œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ í‰ì†Œ ì‚¬ìƒê³¼ í†µì°°ë ¥ì— ì˜ì¡´í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."

    # --- 6. ìµœê·¼ ëŒ€í™” ì •ë¦¬ ---
    formatted_history = [
        {"role": msg["role"], "content": msg["content"]}
        for msg in chat_history[-6:] # í† í° ì ˆì•½ì„ ìœ„í•´ 10 -> 6 ì •ë„ë¡œ ì¡°ì ˆ (ì„ íƒì‚¬í•­)
    ]
    print(rag_prompt)

    # --- 7. ìµœì¢… ë©”ì‹œì§€ êµ¬ì„± ---
    system_message = {
        "role": "system",
        "content": f"{config['system_prompt']}\n\n[RAG ì§€ì¹¨]\n{rag_prompt}"
    }

    messages_to_send = [system_message] + formatted_history + [{"role": "user", "content": user_input}]

    # --- 8. API í˜¸ì¶œ ---
    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=messages_to_send,
        temperature=0.7 
        # ë‹ˆì²´ì˜ ê²½ìš° ì°½ì˜ì„±ì„ ìœ„í•´ temperatureë¥¼ 0.8~0.9ë¡œ ë†’ì´ëŠ” ê²ƒë„ ë°©ë²•ì…ë‹ˆë‹¤.
    )

    return response.choices[0].message.content, source_info

def extract_bible_ref_with_simple_llm(filename: str):
    print("extract_bible_ref_with_simple_llm")

    """
    simple_LLMì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ëª…ì—ì„œ 'ì„±ê²½:ì¥' í˜•íƒœì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì˜ˆ: 'ë§ˆê°€ë³µìŒ 14ì¥' -> 'ë§ˆ:14ì¥'
    """
    
    prompt = ChatPromptTemplate.from_template("""
    ë„ˆëŠ” ë¬¸ìì—´ ë³€í™˜ê¸°ë‹¤.
    ì•„ë˜ íŒŒì¼ëª…ì—ì„œ ì„±ê²½ ê¶Œ ì´ë¦„ê³¼ ì¥ë§Œ ì¶”ì¶œí•˜ì—¬
    ì•„ë˜ ê·œê²©ì˜ ë¬¸ìì—´ë§Œ ì¶œë ¥í•˜ë¼.
    
    ê·œê²©:
    - í˜•ì‹: [ì„±ê²½ í•œê¸€ ì¤„ì„í‘œ]:[ì¥]
    - ì˜ˆì‹œ:
      "ë§ˆê°€ë³µìŒ 14ì¥" â†’ ë§ˆ:14ì¥
      "ì°½ì„¸ê¸° 1ì¥" â†’ ì°½:1ì¥
      "ìš”í•œë³µìŒ 3ì¥ 16ì ˆ" â†’ ìš”:3ì¥
      ì¶”ì¶œ ë¶ˆê°€ â†’ unknown
    
    âš ï¸ ê·œì¹™:
    - ì„¤ëª…í•˜ì§€ ë§ˆë¼
    - ë”°ì˜´í‘œ, ë³„í‘œ(**), ì¤„ë°”ê¿ˆ ì—†ì´
    - ê²°ê³¼ ë¬¸ìì—´ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ë¼
    
    íŒŒì¼ëª…: {filename}
    """)

    
    chain = prompt | simple_llm | StrOutputParser()
    
    try:
        # íŒŒì¼ëª…ì—ì„œ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ê°€ í•µì‹¬ì´ë¯€ë¡œ ì´ ë¶€ë¶„ì„ ê°•ì¡°í•´ì„œ ì „ë‹¬
        bible_ref = chain.invoke({"filename": filename})
        return bible_ref.strip()
    except Exception as e:
        print(f"simple_LLM ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return "unknown"

def expect_query_bible_refs(simple_llm, question: str) -> list[str]:
    print("expect_query_bible_refs")

    prompt = f"""
    ë‹¤ìŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„±ê²½ ê¶Œê³¼ ì¥ì„ JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•´ë¼.
    ì„¤ëª…í•˜ì§€ ë§ˆë¼.

    ì˜ˆ:
    ["ë§ˆ:14ì¥", "ì‹œ:23í¸"]
    ë˜ëŠ”
    []

    ì§ˆë¬¸: {question}
    """
    result = simple_llm.invoke(prompt)
    return parse_list(result)  # ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸


def get_refined_docs(user_input, docs_meta, docs_all, simple_llm):
    print("get_refined_docs")
    # 1. ë¬¸ì„œ í•©ì¹˜ê¸° ë° ì¤‘ë³µ ì œê±°
    all_candidates = docs_meta + docs_all
    unique_docs = {doc.page_content: doc for doc in all_candidates}.values()
    
    # 2. simple_LLMì—ê²Œ ì í•©ì„± íŒë‹¨ ìš”ì²­
    context_text = "\n\n".join([f"[{i+1}] {d.page_content[:500]}..." for i, d in enumerate(unique_docs)])
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì„¤êµ ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ í›„ë³´ë“¤ì„ ë³´ê³ , 
    ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ì •ë§ë¡œ ë„ì›€ì´ ë˜ëŠ” ë¬¸ì„œì˜ ë²ˆí˜¸ë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
    ê´€ë ¨ì´ ì—†ëŠ” ë¬¸ì„œëŠ” ê³¼ê°íˆ ì œì™¸í•˜ì„¸ìš”.
    
    ì§ˆë¬¸: {user_input}
    
    í›„ë³´ ë¬¸ì„œ:
    {context_text}
    
    ì‘ë‹µ í˜•ì‹: ê´€ë ¨ ìˆëŠ” ë¬¸ì„œì˜ ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì ì–´ì£¼ì„¸ìš” (ì˜ˆ: 1, 3). 
    ë§Œì•½ ëª¨ë“  ë¬¸ì„œê°€ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'None'ì´ë¼ê³  ì ì–´ì£¼ì„¸ìš”.
    """
    
    selected_indices = simple_llm.invoke(prompt).content.strip()
    
    if "None" in selected_indices or not selected_indices:
        return list(unique_docs)[:2] # ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ê¸°ë³¸ ìœ ì‚¬ë„ ìƒìœ„ 2ê°œ ë°˜í™˜
    
    # 3. ì„ íƒëœ ë¬¸ì„œë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜
    refined_docs = []
    try:
        indices = [int(i.strip()) - 1 for i in selected_indices.split(",")]
        for idx in indices:
            if 0 <= idx < len(unique_docs):
                refined_docs.append(list(unique_docs)[idx])
    except:
        return list(unique_docs)[:2]

    print(refined_docs)
    return refined_docs
