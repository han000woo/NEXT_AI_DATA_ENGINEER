import os
from pathlib import Path
from abc import ABC, abstractmethod
from typing import List, Tuple, Optional

import openai
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ (ê°€ì •)
from enums.target import TARGET_CONFIG, AnswerTarget, SermonState
from utils.util import parse_list

# --- í™˜ê²½ ì„¤ì • ---
BASE_DIR = Path(__file__).resolve().parents[1]
CONFIG_PATH = BASE_DIR / "config" / ".env"
load_dotenv(CONFIG_PATH)

# --- ì „ì—­ ì„¤ì • (ì‹±ê¸€í†¤ì²˜ëŸ¼ ì‚¬ìš©) ---
# ì„ë² ë”© ëª¨ë¸ì€ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
EMBEDDINGS = OpenAIEmbeddings(model="text-embedding-3-small")
DB_PATH = "chroma_vector_db"


# ==========================================
# [ë¶€ëª¨ í´ë˜ìŠ¤] ê¸°ë³¸ ì±„íŒ… ì„œë¹„ìŠ¤
# ==========================================
class BaseChatService(ABC):
    def __init__(self, target: AnswerTarget):
        self.target = target
        self.config = TARGET_CONFIG[target]
        self.author_name = self.config.get("name", "AI")

        # LLM ëª¨ë¸ ì„¤ì •
        self.main_llm = openai  # OpenAI Client directly
        self.simple_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        # VectorStore ì„¤ì • (ìì‹ì—ì„œ êµ¬ì²´í™”)
        self.vectorstore = self._load_vectorstore()
        self.meta_key = self._get_meta_key()

    @abstractmethod
    def _load_vectorstore(self) -> Chroma:
        """ìì‹ í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•  VectorDBë¥¼ ì •ì˜í•´ì•¼ í•¨"""
        pass

    @abstractmethod
    def _get_meta_key(self) -> str:
        """ì¶œì²˜ í‘œì‹œë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° í‚¤ (title, source, full_ref ë“±)"""
        pass

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        """
        ê¸°ë³¸ ê²€ìƒ‰ ë¡œì§.
        í•„ìš”ì‹œ ìì‹ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë”©(ë®ì–´ì“°ê¸°)í•˜ì—¬ í•„í„°ë§ ë¡œì§ ì¶”ê°€.
        ë°˜í™˜: (docs_llm, docs_all)
        """

        print("_retrieve_documents")
        # ê¸°ë³¸ì€ í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰
        docs_all = self.vectorstore.similarity_search(user_input, k=3)
        return [], docs_all

    def _refine_documents(self, user_input: str, docs_candidates: List) -> List:
        """LLMì„ ì´ìš©í•´ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ê³µí†µ ë¡œì§)"""

        print("_refine_documents")

        if not docs_candidates:
            return []

        # ì¤‘ë³µ ì œê±°
        unique_docs = {doc.page_content: doc for doc in docs_candidates}.values()

        context_text = "\n\n".join(
            [f"[{i+1}] {d.page_content[:500]}..." for i, d in enumerate(unique_docs)]
        )

        prompt = f"""
        ë‹¹ì‹ ì€ ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ì •ë§ë¡œ ë„ì›€ì´ ë˜ëŠ” ë¬¸ì„œ ë²ˆí˜¸ë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
        ì§ˆë¬¸: {user_input}
        í›„ë³´ ë¬¸ì„œ:
        {context_text}
        ì‘ë‹µ í˜•ì‹: ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì˜ˆ: 1, 3). ì—†ìœ¼ë©´ 'None'.
        """

        try:
            selected_indices = self.simple_llm.invoke(prompt).content.strip()
            if "None" in selected_indices or not selected_indices:
                return list(unique_docs)[:2]

            indices = [int(i.strip()) - 1 for i in selected_indices.split(",")]
            refined = [
                list(unique_docs)[idx] for idx in indices if 0 <= idx < len(unique_docs)
            ]
            return refined if refined else list(unique_docs)[:2]
        except Exception as e:
            print(f"Refine Error: {e}")
            return list(unique_docs)[:2]

    def _format_source(self, docs) -> str:
        """ì¶œì²˜ í¬ë§·íŒ… (ìì‹ì—ì„œ ì»¤ìŠ¤í…€ ê°€ëŠ¥)"""
        sources = [doc.metadata.get(self.meta_key, "ì¶œì²˜ ë¯¸ìƒ") for doc in docs]
        unique_sources = list(set(sources))
        return f"ğŸ“– {self.author_name} ì¸ìš©: {', '.join(unique_sources)}"

    def get_response(
        self, user_input: str, chat_history: list
    ) -> Tuple[str, Tuple[SermonState, str]]:
        print(f"[{self.author_name}] get_response ì‹œì‘")

        # 1. ë¬¸ì„œ ê²€ìƒ‰ (ìì‹ í´ë˜ìŠ¤ ë¡œì§ì— ë”°ë¼ ë‹¤ë¦„)
        docs_llm, docs_all = self._retrieve_documents(user_input)
        
        # 2. ë¬¸ì„œ ì •ì œ (Reranking)
        all_candidates = docs_llm + docs_all
        refined_docs = self._refine_documents(user_input, all_candidates)

        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if refined_docs:
            context_text = "\n\n".join([doc.page_content for doc in refined_docs])
            source_str = self._format_source(refined_docs)
            source_info = (SermonState.FOUND, source_str)

            rag_prompt = (
                f"ë‹¤ìŒ ì§€ì‹ ë² ì´ìŠ¤(Context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n"
                f"---\n{context_text}\n---\n"
                f"ì§€ì‹ ë² ì´ìŠ¤ ë‚´ìš©ì„ ë‹¹ì‹ ì˜ ì‚¬ìƒê³¼ ì—°ê²°í•˜ì—¬ í•´ì„í•˜ì„¸ìš”."
            )
        else:
            context_text = ""
            source_info = (SermonState.NOT_FOUND, "")
            rag_prompt = (
                "ê´€ë ¨ ë¬¸í—Œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ í‰ì†Œ í†µì°°ë ¥ì— ì˜ì¡´í•´ ë‹µë³€í•˜ì„¸ìš”."
            )

        # 4. LLM í˜¸ì¶œ
        formatted_history = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in chat_history[-6:]
        ]

        system_message = {
            "role": "system",
            "content": f"{self.config['system_prompt']}\n\n[RAG ì§€ì¹¨]\n{rag_prompt}",
        }

        response = self.main_llm.chat.completions.create(
            model="gpt-4o",
            messages=[system_message]
            + formatted_history
            + [{"role": "user", "content": user_input}],
            temperature=0.7,
        )

        return response.choices[0].message.content, source_info


# ==========================================
# [ìì‹ í´ë˜ìŠ¤ 1] ëª©íšŒì ì„œë¹„ìŠ¤ (ì„±ê²½ í•„í„°ë§ í¬í•¨)
# ==========================================
class PastorService(BaseChatService):
    def __init__(self, target: AnswerTarget, collection_name: str):
        self.collection_name = collection_name
        super().__init__(target)

    def _load_vectorstore(self) -> Chroma:
        return Chroma(
            persist_directory=DB_PATH,
            embedding_function=EMBEDDINGS,
            collection_name=self.collection_name,
        )

    def _get_meta_key(self) -> str:
        return "title"  # ì„¤êµ ì œëª© í‚¤

    def _query_to_vector_search(self, question: str) -> list[str]:
        """ëª©íšŒì íŠ¹í™” ê¸°ëŠ¥: ì§ˆë¬¸ì—ì„œ ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ"""
        prompt = f"""
        ë„ˆëŠ” ì„±ê²½ ë§ì”€ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ
        "ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ë¬¸ì¥"ì„ ë§Œë“œëŠ” ì—­í• ì´ë‹¤.

        ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì½ê³ ,
        ì„±ê²½ êµ¬ì ˆì˜ í•µì‹¬ ì£¼ì œì™€ ìƒí™©ì´ ì˜ ë“œëŸ¬ë‚˜ë„ë¡
        ê²€ìƒ‰ì— ì í•©í•œ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ë°”ê¿”ë¼.

        ê·œì¹™:
        - ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        - ì„±ê²½ êµ¬ì ˆì„ ì§ì ‘ ì¸ìš©í•˜ì§€ ë§ ê²ƒ
        - í•´ì„¤ì´ë‚˜ ì„¤êµì²´ í‘œí˜„ì„ ì“°ì§€ ë§ ê²ƒ
        - ì¶”ìƒì  ìš”ì•½ì´ ì•„ë‹Œ ì˜ë¯¸ì™€ ìƒí™©ì„ ë‹´ì„ ê²ƒ
        - ê²°ê³¼ë§Œ ì¶œë ¥í•  ê²ƒ (ë”°ì˜´í‘œ, ë²ˆí˜¸, ì„¤ëª… ì—†ì´)

        ì‚¬ìš©ì ì§ˆë¬¸:
        {question}
        """

        result = self.simple_llm.invoke(prompt).content
        return result

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        # 1. ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ
        query_to_vector_search = self._query_to_vector_search(user_input)

        print(f"query_to_vector_search {query_to_vector_search}")
        docs_llm = []
        # ë§Œì•½ ì°¸ì¡°í•œ ì„±ê²½ì´ ê²€ìƒ‰ì´ ëœë‹¤ë©´
        if query_to_vector_search:
            print(f"ğŸ” ì„±ê²½ í•„í„° ì ìš©: {query_to_vector_search}")
            docs_llm = self.vectorstore.similarity_search(user_input, k=3)

        docs_all = self.vectorstore.similarity_search(user_input, k=3)
        print(docs_llm)
        
        return docs_llm, docs_all

    def _format_source(self, docs) -> str:
        # ë¶€ëª¨ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ëª©ì‚¬ë‹˜ ì „ìš© ë¬¸êµ¬ ì‚¬ìš©
        sources = [doc.metadata.get(self.meta_key, "ì œëª© ë¯¸ìƒ") for doc in docs]
        unique_sources = list(set(sources))
        return f"ğŸ“– {self.author_name} ì„¤êµ: {', '.join(unique_sources)}"


# ==========================================
# [ìì‹ í´ë˜ìŠ¤ 2] ë‹ˆì²´ ì„œë¹„ìŠ¤ (ì² í•™ì  ì ‘ê·¼)
# ==========================================
class NietzscheService(BaseChatService):
    def _load_vectorstore(self) -> Chroma:
        return Chroma(
            persist_directory=DB_PATH,
            embedding_function=EMBEDDINGS,
            collection_name="nietzsche_works",
        )

    def _get_meta_key(self) -> str:
        return "chapter_title"  # ë‹ˆì²´ ì €ì„œ ì¸ìš© í‚¤

    def _translate_to_english(self, question: str) -> str:
        """ëª©íšŒì íŠ¹í™” ê¸°ëŠ¥: ì§ˆë¬¸ì—ì„œ ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ"""
        prompt = f"""
        ì˜ë¬¸ìœ¼ë¡œ ë²ˆì—­í•˜ê³  ë²ˆì—­í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•´ì¤˜
        ì§ˆë¬¸: {question}
        """
        result = self.simple_llm.invoke(prompt).content
        return result

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        # ë‹ˆì²´ëŠ” ì„±ê²½ í•„í„°ë§ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë‹¨ìˆœ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        # (í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì² í•™ ìš©ì–´ í•„í„°ë§ ë“±ì„ ì¶”ê°€ ê°€ëŠ¥)
        user_input = self._translate_to_english(user_input)
        print(user_input)
        return [], self.vectorstore.similarity_search(
            user_input, k=4
        )  # kë¥¼ ì¡°ê¸ˆ ëŠ˜ë¦¼

    def _format_source(self, docs) -> str:
        sources = [doc.metadata.get(self.meta_key, "ì¶œì²˜ ë¯¸ìƒ") for doc in docs]
        unique_sources = list(set(sources))
        return f"ğŸ“œ ë‹ˆì²´ ì €ì„œ ì¸ìš©: {', '.join(unique_sources)}"

# ==========================================
# [ìì‹ í´ë˜ìŠ¤ 3] ë²•ë¥œ ì„œë¹„ìŠ¤ (ë¶ˆêµì  ì ‘ê·¼)
# ==========================================
class BubryuneService(BaseChatService):
    def _load_vectorstore(self) -> Chroma:
        return Chroma(
            persist_directory=DB_PATH,
            embedding_function=EMBEDDINGS,
            collection_name="bubryune_works",
        )

    def _get_meta_key(self) -> str:
        return "chapter_title"  # ë‹ˆì²´ ì €ì„œ ì¸ìš© í‚¤

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        # ë²•ë¥œìŠ¤ë‹˜ì€ ë‹¨ìˆœ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        # (í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì² í•™ ìš©ì–´ í•„í„°ë§ ë“±ì„ ì¶”ê°€ ê°€ëŠ¥)
        print(user_input)
        return [], self.vectorstore.similarity_search(
            user_input, k=4
        )  # kë¥¼ ì¡°ê¸ˆ ëŠ˜ë¦¼

    def _format_source(self, docs) -> str:
        sources = [doc.metadata.get(self.meta_key, "ì¶œì²˜ ë¯¸ìƒ") for doc in docs]
        unique_sources = list(set(sources))
        return f"ë²•ë¥œìŠ¤ë‹˜ ë²•ë¬¸ ì¸ìš©: {', '.join(unique_sources)}"

# ==========================================
# [íŒ©í† ë¦¬] ì„œë¹„ìŠ¤ ìƒì„±ê¸°
# ==========================================
def get_chat_service(target: AnswerTarget) -> BaseChatService:
    if target == AnswerTarget.PASTOR_A:
        return PastorService(target, collection_name="yujin_works")

    elif target == AnswerTarget.PASTOR_B:
        return PastorService(target, collection_name="woonsung_works")

    elif target == AnswerTarget.NIETZSCHE:
        return NietzscheService(target)

    elif target == AnswerTarget.BUBRYUNE :
        return BubryuneService(target)
    else:
        # ê¸°ë³¸ê°’ (ì˜ˆì™¸ ì²˜ë¦¬)
        return PastorService(AnswerTarget.PASTOR_A, collection_name="yujin_works")


# ==========================================
# [ë©”ì¸ ì‹¤í–‰ë¶€] ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´
# ==========================================
def get_response(user_input, chat_history, target: AnswerTarget):
    # 1. íƒ€ê²Ÿì— ë§ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± (íŒ©í† ë¦¬ íŒ¨í„´)
    service = get_chat_service(target)

    # 2. ê°ì²´ì˜ ë©”ì„œë“œ ì‹¤í–‰
    return service.get_response(user_input, chat_history)
