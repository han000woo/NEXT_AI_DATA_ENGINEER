import os
from pathlib import Path
from abc import ABC, abstractmethod
from typing import List, Tuple, Optional

import openai
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ (ê°€ì •)
from enums.target import TARGET_CONFIG, AnswerTarget, SermonState
from utils.util import parse_list

# --- í™˜ê²½ ì„¤ì • ---
BASE_DIR = Path(__file__).resolve().parents[1]
CONFIG_PATH = BASE_DIR / "config" / ".env"
load_dotenv(CONFIG_PATH)

# --- ì „ì—­ ì„¤ì • (ì‹±ê¸€í†¤ì²˜ëŸ¼ ì‚¬ìš©) ---
# ì„ë² ë”© ëª¨ë¸ì€ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
EMBEDDINGS = OpenAIEmbeddings(model="text-embedding-3-small")
DB_PATH = "chroma_vector_db"

# ==========================================
# [ë¶€ëª¨ í´ë˜ìŠ¤] ê¸°ë³¸ ì±„íŒ… ì„œë¹„ìŠ¤
# ==========================================
class BaseChatService(ABC):
    def __init__(self, target: AnswerTarget):
        self.target = target
        self.config = TARGET_CONFIG[target]
        self.author_name = self.config.get("name", "AI")
        
        # LLM ëª¨ë¸ ì„¤ì •
        self.main_llm = openai  # OpenAI Client directly
        self.simple_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        # VectorStore ì„¤ì • (ìì‹ì—ì„œ êµ¬ì²´í™”)
        self.vectorstore = self._load_vectorstore()
        self.meta_key = self._get_meta_key()

    @abstractmethod
    def _load_vectorstore(self) -> Chroma:
        """ìì‹ í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•  VectorDBë¥¼ ì •ì˜í•´ì•¼ í•¨"""
        pass

    @abstractmethod
    def _get_meta_key(self) -> str:
        """ì¶œì²˜ í‘œì‹œë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° í‚¤ (title, source, full_ref ë“±)"""
        pass

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        """
        ê¸°ë³¸ ê²€ìƒ‰ ë¡œì§. 
        í•„ìš”ì‹œ ìì‹ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë”©(ë®ì–´ì“°ê¸°)í•˜ì—¬ í•„í„°ë§ ë¡œì§ ì¶”ê°€.
        ë°˜í™˜: (docs_meta, docs_all)
        """
        
        print("_retrieve_documents")
        # ê¸°ë³¸ì€ í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰
        docs_all = self.vectorstore.similarity_search(user_input, k=3)
        return [], docs_all

    def _refine_documents(self, user_input: str, docs_candidates: List) -> List:
        """LLMì„ ì´ìš©í•´ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ê³µí†µ ë¡œì§)"""

        print("_refine_documents")

        if not docs_candidates:
            return []

        # ì¤‘ë³µ ì œê±°
        unique_docs = {doc.page_content: doc for doc in docs_candidates}.values()
        
        context_text = "\n\n".join([f"[{i+1}] {d.page_content[:500]}..." for i, d in enumerate(unique_docs)])
        
        prompt = f"""
        ë‹¹ì‹ ì€ ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ì •ë§ë¡œ ë„ì›€ì´ ë˜ëŠ” ë¬¸ì„œ ë²ˆí˜¸ë§Œ ê³¨ë¼ì£¼ì„¸ìš”.
        ì§ˆë¬¸: {user_input}
        í›„ë³´ ë¬¸ì„œ:
        {context_text}
        ì‘ë‹µ í˜•ì‹: ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„ (ì˜ˆ: 1, 3). ì—†ìœ¼ë©´ 'None'.
        """
        
        try:
            selected_indices = self.simple_llm.invoke(prompt).content.strip()
            if "None" in selected_indices or not selected_indices:
                return list(unique_docs)[:2]
            
            indices = [int(i.strip()) - 1 for i in selected_indices.split(",")]
            refined = [list(unique_docs)[idx] for idx in indices if 0 <= idx < len(unique_docs)]
            return refined if refined else list(unique_docs)[:2]
        except Exception as e:
            print(f"Refine Error: {e}")
            return list(unique_docs)[:2]

    def _format_source(self, docs) -> str:
        """ì¶œì²˜ í¬ë§·íŒ… (ìì‹ì—ì„œ ì»¤ìŠ¤í…€ ê°€ëŠ¥)"""
        sources = [doc.metadata.get(self.meta_key, 'ì¶œì²˜ ë¯¸ìƒ') for doc in docs]
        unique_sources = list(set(sources))
        return f"ğŸ“– {self.author_name} ì¸ìš©: {', '.join(unique_sources)}"

    def get_response(self, user_input: str, chat_history: list) -> Tuple[str, Tuple[SermonState, str]]:
        print(f"[{self.author_name}] get_response ì‹œì‘")
        
        # 1. ë¬¸ì„œ ê²€ìƒ‰ (ìì‹ í´ë˜ìŠ¤ ë¡œì§ì— ë”°ë¼ ë‹¤ë¦„)
        docs_meta, docs_all = self._retrieve_documents(user_input)
        
        # 2. ë¬¸ì„œ ì •ì œ (Reranking)
        all_candidates = docs_meta + docs_all
        refined_docs = self._refine_documents(user_input, all_candidates)

        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if refined_docs and docs_meta:
            context_text = "\n\n".join([doc.page_content for doc in refined_docs])
            source_str = self._format_source(refined_docs)
            source_info = (SermonState.FOUND, source_str)
            
            rag_prompt = (
                f"ë‹¤ìŒ ì§€ì‹ ë² ì´ìŠ¤(Context)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n"
                f"---\n{context_text}\n---\n"
                f"ì§€ì‹ ë² ì´ìŠ¤ ë‚´ìš©ì„ ë‹¹ì‹ ì˜ ì‚¬ìƒê³¼ ì—°ê²°í•˜ì—¬ í•´ì„í•˜ì„¸ìš”."
            )
        else:
            context_text = ""
            source_info = (SermonState.NOT_FOUND, "")
            rag_prompt = "ê´€ë ¨ ë¬¸í—Œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ í‰ì†Œ í†µì°°ë ¥ì— ì˜ì¡´í•´ ë‹µë³€í•˜ì„¸ìš”."

        # 4. LLM í˜¸ì¶œ
        formatted_history = [
            {"role": msg["role"], "content": msg["content"]} 
            for msg in chat_history[-6:]
        ]
        
        system_message = {
            "role": "system",
            "content": f"{self.config['system_prompt']}\n\n[RAG ì§€ì¹¨]\n{rag_prompt}"
        }

        response = self.main_llm.chat.completions.create(
            model="gpt-4o",
            messages=[system_message] + formatted_history + [{"role": "user", "content": user_input}],
            temperature=0.7
        )
        
        return response.choices[0].message.content, source_info


# ==========================================
# [ìì‹ í´ë˜ìŠ¤ 1] ëª©íšŒì ì„œë¹„ìŠ¤ (ì„±ê²½ í•„í„°ë§ í¬í•¨)
# ==========================================
class PastorService(BaseChatService):
    def __init__(self, target: AnswerTarget, collection_name: str):
        self.collection_name = collection_name
        super().__init__(target)

    def _load_vectorstore(self) -> Chroma:
        return Chroma(
            persist_directory=DB_PATH,
            embedding_function=EMBEDDINGS,
            collection_name=self.collection_name
        )

    def _get_meta_key(self) -> str:
        return 'title' # ì„¤êµ ì œëª© í‚¤

    def _expect_query_bible_refs(self, question: str) -> list[str]:
        """ëª©íšŒì íŠ¹í™” ê¸°ëŠ¥: ì§ˆë¬¸ì—ì„œ ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ"""
        prompt = f"""
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì„±ê²½ ê¶Œê³¼ ì¥ì„ JSON ë°°ì—´ë¡œ ì¶œë ¥. ì˜ˆ: ["ë§ˆ:14ì¥"] ë˜ëŠ” [].
        ì§ˆë¬¸: {question}
        """
        result = self.simple_llm.invoke(prompt).content
        return parse_list(result)

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        # 1. ì„±ê²½ êµ¬ì ˆ ì¶”ì¶œ
        bible_refs = self._expect_query_bible_refs(user_input)

        print(f"_retrieve_documents {bible_refs}")
        docs_meta = []
        if bible_refs:
            print(f"ğŸ” ì„±ê²½ í•„í„° ì ìš©: {bible_refs}")
            docs_meta = self.vectorstore.similarity_search(
                user_input, k=3, filter={"bible_ref": {"$in": bible_refs}}
            )
            
        docs_all = self.vectorstore.similarity_search(user_input, k=3)
        return docs_meta, docs_all
    
    def _format_source(self, docs) -> str:
        # ë¶€ëª¨ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ ëª©ì‚¬ë‹˜ ì „ìš© ë¬¸êµ¬ ì‚¬ìš©
        sources = [doc.metadata.get(self.meta_key, 'ì œëª© ë¯¸ìƒ') for doc in docs]
        unique_sources = list(set(sources))
        return f"ğŸ“– {self.author_name} ì„¤êµ: {', '.join(unique_sources)}"


# ==========================================
# [ìì‹ í´ë˜ìŠ¤ 2] ë‹ˆì²´ ì„œë¹„ìŠ¤ (ì² í•™ì  ì ‘ê·¼)
# ==========================================
class NietzscheService(BaseChatService):
    def _load_vectorstore(self) -> Chroma:
        return Chroma(
            persist_directory=DB_PATH,
            embedding_function=EMBEDDINGS,
            collection_name='nietzsche_works'
        )

    def _get_meta_key(self) -> str:
        return 'full_ref' # ë‹ˆì²´ ì €ì„œ ì¸ìš© í‚¤

    def _retrieve_documents(self, user_input: str) -> Tuple[List, List]:
        # ë‹ˆì²´ëŠ” ì„±ê²½ í•„í„°ë§ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ë‹¨ìˆœ ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        # (í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì² í•™ ìš©ì–´ í•„í„°ë§ ë“±ì„ ì¶”ê°€ ê°€ëŠ¥)
        return [], self.vectorstore.similarity_search(user_input, k=4) # kë¥¼ ì¡°ê¸ˆ ëŠ˜ë¦¼

    def _format_source(self, docs) -> str:
        sources = [doc.metadata.get(self.meta_key, 'ì¶œì²˜ ë¯¸ìƒ') for doc in docs]
        unique_sources = list(set(sources))
        return f"ğŸ“œ ë‹ˆì²´ ì €ì„œ ì¸ìš©: {', '.join(unique_sources)}"


# ==========================================
# [íŒ©í† ë¦¬] ì„œë¹„ìŠ¤ ìƒì„±ê¸°
# ==========================================
def get_chat_service(target: AnswerTarget) -> BaseChatService:
    if target == AnswerTarget.PASTOR_A:
        return PastorService(target, collection_name='yujin_works')
    
    elif target == AnswerTarget.PASTOR_B:
        return PastorService(target, collection_name='woonsung_works')
    
    elif target == AnswerTarget.NIETZSCHE:
        return NietzscheService(target)
    
    else:
        # ê¸°ë³¸ê°’ (ì˜ˆì™¸ ì²˜ë¦¬)
        return PastorService(AnswerTarget.PASTOR_A, collection_name='yujin_works')


# ==========================================
# [ë©”ì¸ ì‹¤í–‰ë¶€] ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´
# ==========================================
def get_response(user_input, chat_history, target: AnswerTarget):
    # 1. íƒ€ê²Ÿì— ë§ëŠ” ì„œë¹„ìŠ¤ ê°ì²´ ìƒì„± (íŒ©í† ë¦¬ íŒ¨í„´)
    service = get_chat_service(target)
    
    # 2. ê°ì²´ì˜ ë©”ì„œë“œ ì‹¤í–‰
    return service.get_response(user_input, chat_history)