import struct
import zlib
import olefile
from pathlib import Path
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from dotenv import load_dotenv

from backend.service import extract_bible_ref_with_llm

BASE_DIR = Path(__file__).resolve().parents[1]
CONFIG_PATH = BASE_DIR / "config" / ".env"

load_dotenv(CONFIG_PATH)


# --- 1. [í•µì‹¬] HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def get_hwp_text(filename):
    f = olefile.OleFileIO(filename)
    dirs = f.listdir()

    if ["FileHeader"] not in dirs or \
            ["\x05HwpSummaryInformation"] not in dirs:
        raise Exception("Not Valid HWP.")

    header = f.openstream("FileHeader")
    header_data = header.read()
    is_compressed = (header_data[36] & 1) == 1

    nums = []
    for d in dirs:
        if d[0] == "BodyText":
            nums.append(int(d[1][len("Section"):]))
    sections = ["BodyText/Section" + str(x) for x in sorted(nums)]

    text = ""
    for section in sections:
        bodytext = f.openstream(section)
        data = bodytext.read()
        if is_compressed:
            unpacked_data = zlib.decompress(data, -15)
        else:
            unpacked_data = data

        section_text = ""
        i = 0
        size = len(unpacked_data)
        while i < size:
            header = struct.unpack_from("<I", unpacked_data, i)[0]
            rec_type = header & 0x3ff
            rec_len = (header >> 20) & 0xfff

            if rec_type in [67]:
                rec_data = unpacked_data[i + 4:i + 4 + rec_len]
                section_text += rec_data.decode('utf-16')
                section_text += "\n"

            i += 4 + rec_len

        text += section_text
        text += "\n"

    return text


# --- 2. ì„¤êµ ë³¸ë¬¸ ì •ì œ (ì œëª©, ì„œë¡ ~ì¶•ë„) ---
def extract_core_sermon(hwp_path):
    
    try:
        full_text = get_hwp_text(hwp_path)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì—ëŸ¬: {hwp_path.name} - {e}")
        return None, None

    if not full_text:
        return None, None

    title = hwp_path.stem # ê¸°ë³¸ê°’ì€ íŒŒì¼ëª…

    return title, full_text.strip()

    
# --- 3. ë¬¸ì„œ ë¡œë“œ ë° ê°ì²´ ìƒì„± ---
def load_woonsung_hwp(hwf_dir): 
    print(f"ğŸ“‚ HWP í´ë” ì½ê¸°: {hwf_dir}")
    documents = [] 
    
    if hwf_dir.exists():
        for hwp_path in hwf_dir.glob("*.hwp"): # .hwp í™•ì¥ì í™•ì¸
            
            title, content = extract_core_sermon(hwp_path)

            if content and len(content) > 50: # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ì€ ìŠ¤í‚µ
                print(f" - [{title}] ë¡œë“œ ì™„ë£Œ ({len(content)}ì)")
                print(f" ğŸ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘: {hwp_path.name}")
                bible_reference = extract_bible_ref_with_llm(hwp_path.name)
                print(f" LLM ìš”ì•½ ë©”íƒ€ë°ì´í„° : {bible_reference}")

                doc = Document(
                    page_content=content,
                    metadata={
                        "source": hwp_path.name,
                        "title": title,         
                        "author": "ì •ìš´ì„± ëª©ì‚¬", 
                        "category": "sermon",
                        "bible_ref": bible_reference  # ê·œê²©í™”ëœ ì •ë³´ ì €ì¥ (ì˜ˆ: ë§ˆ:14ì¥)
                    }
                )
                documents.append(doc)
            else:
                print(f" âš ï¸ ìŠ¤í‚µ: {hwp_path.name} (ë‚´ìš© ì—†ìŒ ë˜ëŠ” íŒ¨í„´ ë¶ˆì¼ì¹˜)")
    else:
        print("âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    return documents

# --- 4. ë©”ì¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def preprocess_woonsung(hwf_dir, persist_directory):
    # ë¬¸ì„œ ë¡œë“œ
    documents = load_woonsung_hwp(hwf_dir)

    if not documents:
        print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, 
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""] 
    )

    split_docs = text_splitter.split_documents(documents)
    print(f"\nâœ‚ï¸ ì´ {len(documents)}ê°œì˜ ì„¤êµë¥¼ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # ë²¡í„° DB ì €ì¥
    if split_docs:
        print("ğŸ’¾ ChromaDBì— ì €ì¥ ì¤‘...")

        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name="woonsung_works" # ì»¬ë ‰ì…˜ ì´ë¦„ êµ¬ë¶„!
        )
        print("âœ… ì €ì¥ ì™„ë£Œ! DBê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")